{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLmYn_4mvMa_",
        "outputId": "9feb59a7-121c-4488-9967-d315210b7d14"
      },
      "outputs": [],
      "source": [
        "!pip install camel-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYVpNp0JvyHl",
        "outputId": "c36e87a6-e2a6-4dc4-a0d2-996ec903b4c1"
      },
      "outputs": [],
      "source": [
        "!camel_data -i all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "LXbJ6QFnv6XP",
        "outputId": "3ed50c91-e94c-48b2-986c-09e9374a00ef"
      },
      "outputs": [],
      "source": [
        "## Example of MorphologicalTokenizer usage\n",
        "\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
        "\n",
        "# Initialize disambiguators\n",
        "mle_msa = MLEDisambiguator.pretrained('calima-msa-r13')\n",
        "mle_egy = MLEDisambiguator.pretrained('calima-egy-r13')\n",
        "\n",
        "# We expect a sentence to be whitespace/punctuation tokenized beforehand.\n",
        "# We provide a simple whitespace and punctuation tokenizer as part of camel_tools.\n",
        "# See camel_tools.tokenizers.word.simple_word_tokenize.\n",
        "sentence_msa = ['فتنفست', 'الصعداء']\n",
        "sentence_egy = ['وكاتباله', 'مكتوبين']\n",
        "\n",
        "# Create different morphological tokenizer instances\n",
        "msa_d3_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, split=True, scheme='d3tok')\n",
        "msa_atb_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, split=True, scheme='atbtok')\n",
        "msa_bw_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, split=True, scheme='bwtok')\n",
        "egy_bw_tokenizer = MorphologicalTokenizer(disambiguator=mle_egy, split=True, scheme='bwtok')\n",
        "\n",
        "# Generate tokenizations\n",
        "# Note that our Egyptian resources currently provide bwtok tokenization only.\n",
        "msa_d3_tok = msa_d3_tokenizer.tokenize(sentence_msa)\n",
        "msa_atb_tok = msa_atb_tokenizer.tokenize(sentence_msa)\n",
        "msa_bw_tok = msa_bw_tokenizer.tokenize(sentence_msa)\n",
        "egy_bw_tok = egy_bw_tokenizer.tokenize(sentence_egy)\n",
        "\n",
        "# Print results\n",
        "print('D3 tokenization (MSA):', msa_d3_tok)\n",
        "print('ATB tokenization (MSA):', msa_atb_tok)\n",
        "print('BW tokenization (MSA):', msa_bw_tok)\n",
        "print('BW tokenization (EGY):', egy_bw_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4w4CDvA8eo7",
        "outputId": "a789f1b2-bf7f-4b5d-a107-e71f03d480b4"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from nltk.corpus import stopwords\n",
        "import utils\n",
        "\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.dialectid import DialectIdentifier\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiJh-ioxNtCg"
      },
      "outputs": [],
      "source": [
        "train_set = pd.read_csv(\"Dataset/train.csv\")\n",
        "dev_set = pd.read_csv(\"Dataset/dev.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## load pre-trained models\n",
        "\n",
        "# dialect identification\n",
        "did = DialectIdentifier.pretrained()\n",
        "\n",
        "# Initialize disambiguators\n",
        "msa_mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
        "egy_mle = MLEDisambiguator.pretrained('calima-egy-r13')\n",
        "#TODO: this stopword list has more words and some of them should be removed from tweets?\n",
        "stopwords_list_cleaned = stopwords.words('arabic')\n",
        "for i in range(len(stopwords_list_cleaned)):\n",
        "  stopwords_list_cleaned[i] = dediac_ar(stopwords_list_cleaned[i])\n",
        "stopwords_list_cleaned = list(dict.fromkeys(stopwords_list_cleaned))\n",
        "\n",
        "dialect_map = {\n",
        "    'Gulf': msa_mle,\n",
        "    'Levant': egy_mle, \n",
        "    'Modern Standard Arabic': msa_mle, \n",
        "    'Maghreb': egy_mle, \n",
        "    'Nile Basin': egy_mle, \n",
        "    'Gulf of Aden': msa_mle\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTY1dDZR2PMs"
      },
      "outputs": [],
      "source": [
        "def preprocess(dataset):\n",
        "  cleaned_dataset = dataset.copy()\n",
        "\n",
        "  cleaned_dataset['text'] = cleaned_dataset['text'].apply(utils.clean_tweet)\n",
        "  cleaned_dataset['text'] = cleaned_dataset['text'].apply(lambda tweet: utils.tokenize_with_dialect(tweet, did, msa_mle=msa_mle, egy_mle=egy_mle))\n",
        "  cleaned_dataset['text'] = cleaned_dataset['text'].apply(utils.normalize_chars)\n",
        "\n",
        "  # merge dup tweets and use the most occuring classes (TODO: doesn't keep the order for some reason ...) \n",
        "  # IMPORTANT: you can comment the following line to be able to compare the results with the original dataset (same order)\n",
        "  cleaned_dataset = cleaned_dataset.groupby(cleaned_dataset['text']).agg(pd.Series.mode)\n",
        "\n",
        "  return cleaned_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDwsSsHO8G4a"
      },
      "outputs": [],
      "source": [
        "cleaned_train_set = preprocess(train_set)\n",
        "\n",
        "# save the cleaned data\n",
        "cleaned_train_set.to_csv(\"./Dataset/cleaned_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTBV5zw2NsrT"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTUcrgw400I0",
        "outputId": "1d45ae30-9120-45da-a760-0b64e2443d4d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
